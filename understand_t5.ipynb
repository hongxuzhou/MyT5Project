{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06c95999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbc8900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5572eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "byt5_tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1a2ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    \"hello, world \", # Correct spelling\n",
    "    \"helo, world\", # Missing \n",
    "    \"hello, worlld\", # Extra letter\n",
    "    \"g8 job\", # Leetspeak\n",
    "    \"café Saarbrücken\", # Accented characters\n",
    "    \"你好, 世界\" # Non-Latin script\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce30ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokenisation(text, verbose=True): \n",
    "    \"\"\"\n",
    "    Compare T5 and ByT5 tokenisation strategies.\n",
    "    \n",
    "    Returns:\n",
    "        dic: contains the detailed tokenisation info.\n",
    "    \"\"\"\n",
    "    # T5 subword tokenization\n",
    "    t5_tokens = t5_tokenizer.tokenize(text)\n",
    "    t5_ids = t5_tokenizer.encode(text)\n",
    "\n",
    "    # ByT5 byte-level tokenization\n",
    "    byt5_tokens = byt5_tokenizer.tokenize(text)\n",
    "    byt5_ids = byt5_tokenizer.encode(text)\n",
    "\n",
    "    # Mapping\n",
    "    byte_mapping = []\n",
    "    for char in text:\n",
    "        byte_val = ord(char)\n",
    "        byte_mapping.append({\n",
    "            \"char\": char,\n",
    "            \"byte\": byte_val,\n",
    "            \"token_id\": byte_val + 3 # ByT5 adds an offset of 3\n",
    "        })\n",
    "    # Organise the return data\n",
    "    results = {\n",
    "        \"text\" : text,\n",
    "        \"t5\": {\n",
    "            \"tokens\": t5_tokens,\n",
    "            \"ids\": t5_ids,\n",
    "            \"count\": len(t5_tokens)\n",
    "        },\n",
    "        \"byt5\": {\n",
    "            \"tokens\": byt5_tokens,\n",
    "            \"ids\": byt5_ids,\n",
    "            \"count\": len(byt5_tokens)\n",
    "        },\n",
    "        \"byte_mapping\": byte_mapping\n",
    "    }\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"原文: {text}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        print(f\"\\nT5 Tokenization:\")\n",
    "        print(f\"  Tokens: {t5_tokens}\")\n",
    "        print(f\"  IDs: {t5_ids}\")\n",
    "        print(f\"  Token count: {len(t5_tokens)}\")\n",
    "        \n",
    "        print(f\"\\nbyT5 Tokenization:\")\n",
    "        print(f\"  Tokens: {byt5_tokens}\")\n",
    "        print(f\"  IDs: {byt5_ids}\")\n",
    "        print(f\"  Token count: {len(byt5_tokens)}\")\n",
    "        \n",
    "        print(f\"\\n字节映射:\")\n",
    "        for item in byte_mapping:\n",
    "            print(f\"  '{item['char']}' -> byte {item['byte']} -> token_id {item['token_id']}\")\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4a054bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "原文: hello, world \n",
      "============================================================\n",
      "\n",
      "T5 Tokenization:\n",
      "  Tokens: ['▁hello', ',', '▁world']\n",
      "  IDs: [21820, 6, 296, 1]\n",
      "  Token count: 3\n",
      "\n",
      "byT5 Tokenization:\n",
      "  Tokens: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', ' ']\n",
      "  IDs: [107, 104, 111, 111, 114, 47, 35, 122, 114, 117, 111, 103, 35, 1]\n",
      "  Token count: 13\n",
      "\n",
      "字节映射:\n",
      "  'h' -> byte 104 -> token_id 107\n",
      "  'e' -> byte 101 -> token_id 104\n",
      "  'l' -> byte 108 -> token_id 111\n",
      "  'l' -> byte 108 -> token_id 111\n",
      "  'o' -> byte 111 -> token_id 114\n",
      "  ',' -> byte 44 -> token_id 47\n",
      "  ' ' -> byte 32 -> token_id 35\n",
      "  'w' -> byte 119 -> token_id 122\n",
      "  'o' -> byte 111 -> token_id 114\n",
      "  'r' -> byte 114 -> token_id 117\n",
      "  'l' -> byte 108 -> token_id 111\n",
      "  'd' -> byte 100 -> token_id 103\n",
      "  ' ' -> byte 32 -> token_id 35\n",
      "\n",
      "============================================================\n",
      "原文: helo, world\n",
      "============================================================\n",
      "\n",
      "T5 Tokenization:\n",
      "  Tokens: ['▁', 'he', 'l', 'o', ',', '▁world']\n",
      "  IDs: [3, 88, 40, 32, 6, 296, 1]\n",
      "  Token count: 6\n",
      "\n",
      "byT5 Tokenization:\n",
      "  Tokens: ['h', 'e', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd']\n",
      "  IDs: [107, 104, 111, 114, 47, 35, 122, 114, 117, 111, 103, 1]\n",
      "  Token count: 11\n",
      "\n",
      "字节映射:\n",
      "  'h' -> byte 104 -> token_id 107\n",
      "  'e' -> byte 101 -> token_id 104\n",
      "  'l' -> byte 108 -> token_id 111\n",
      "  'o' -> byte 111 -> token_id 114\n",
      "  ',' -> byte 44 -> token_id 47\n",
      "  ' ' -> byte 32 -> token_id 35\n",
      "  'w' -> byte 119 -> token_id 122\n",
      "  'o' -> byte 111 -> token_id 114\n",
      "  'r' -> byte 114 -> token_id 117\n",
      "  'l' -> byte 108 -> token_id 111\n",
      "  'd' -> byte 100 -> token_id 103\n",
      "\n",
      "============================================================\n",
      "原文: hello, worlld\n",
      "============================================================\n",
      "\n",
      "T5 Tokenization:\n",
      "  Tokens: ['▁hello', ',', '▁wo', 'r', 'll', 'd']\n",
      "  IDs: [21820, 6, 2275, 52, 195, 26, 1]\n",
      "  Token count: 6\n",
      "\n",
      "byT5 Tokenization:\n",
      "  Tokens: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'l', 'd']\n",
      "  IDs: [107, 104, 111, 111, 114, 47, 35, 122, 114, 117, 111, 111, 103, 1]\n",
      "  Token count: 13\n",
      "\n",
      "字节映射:\n",
      "  'h' -> byte 104 -> token_id 107\n",
      "  'e' -> byte 101 -> token_id 104\n",
      "  'l' -> byte 108 -> token_id 111\n",
      "  'l' -> byte 108 -> token_id 111\n",
      "  'o' -> byte 111 -> token_id 114\n",
      "  ',' -> byte 44 -> token_id 47\n",
      "  ' ' -> byte 32 -> token_id 35\n",
      "  'w' -> byte 119 -> token_id 122\n",
      "  'o' -> byte 111 -> token_id 114\n",
      "  'r' -> byte 114 -> token_id 117\n",
      "  'l' -> byte 108 -> token_id 111\n",
      "  'l' -> byte 108 -> token_id 111\n",
      "  'd' -> byte 100 -> token_id 103\n",
      "\n",
      "============================================================\n",
      "原文: g8 job\n",
      "============================================================\n",
      "\n",
      "T5 Tokenization:\n",
      "  Tokens: ['▁', 'g', '8', '▁job']\n",
      "  IDs: [3, 122, 927, 613, 1]\n",
      "  Token count: 4\n",
      "\n",
      "byT5 Tokenization:\n",
      "  Tokens: ['g', '8', ' ', 'j', 'o', 'b']\n",
      "  IDs: [106, 59, 35, 109, 114, 101, 1]\n",
      "  Token count: 6\n",
      "\n",
      "字节映射:\n",
      "  'g' -> byte 103 -> token_id 106\n",
      "  '8' -> byte 56 -> token_id 59\n",
      "  ' ' -> byte 32 -> token_id 35\n",
      "  'j' -> byte 106 -> token_id 109\n",
      "  'o' -> byte 111 -> token_id 114\n",
      "  'b' -> byte 98 -> token_id 101\n",
      "\n",
      "============================================================\n",
      "原文: café Saarbrücken\n",
      "============================================================\n",
      "\n",
      "T5 Tokenization:\n",
      "  Tokens: ['▁café', '▁Sa', 'ar', 'brücke', 'n']\n",
      "  IDs: [11949, 1138, 291, 25892, 29, 1]\n",
      "  Token count: 5\n",
      "\n",
      "byT5 Tokenization:\n",
      "  Tokens: ['c', 'a', 'f', 'Ã', '©', ' ', 'S', 'a', 'a', 'r', 'b', 'r', 'Ã', '¼', 'c', 'k', 'e', 'n']\n",
      "  IDs: [102, 100, 105, 198, 172, 35, 86, 100, 100, 117, 101, 117, 198, 191, 102, 110, 104, 113, 1]\n",
      "  Token count: 18\n",
      "\n",
      "字节映射:\n",
      "  'c' -> byte 99 -> token_id 102\n",
      "  'a' -> byte 97 -> token_id 100\n",
      "  'f' -> byte 102 -> token_id 105\n",
      "  'é' -> byte 233 -> token_id 236\n",
      "  ' ' -> byte 32 -> token_id 35\n",
      "  'S' -> byte 83 -> token_id 86\n",
      "  'a' -> byte 97 -> token_id 100\n",
      "  'a' -> byte 97 -> token_id 100\n",
      "  'r' -> byte 114 -> token_id 117\n",
      "  'b' -> byte 98 -> token_id 101\n",
      "  'r' -> byte 114 -> token_id 117\n",
      "  'ü' -> byte 252 -> token_id 255\n",
      "  'c' -> byte 99 -> token_id 102\n",
      "  'k' -> byte 107 -> token_id 110\n",
      "  'e' -> byte 101 -> token_id 104\n",
      "  'n' -> byte 110 -> token_id 113\n",
      "\n",
      "============================================================\n",
      "原文: 你好, 世界\n",
      "============================================================\n",
      "\n",
      "T5 Tokenization:\n",
      "  Tokens: ['▁', '你好', ',', '▁', '世界']\n",
      "  IDs: [3, 2, 6, 3, 2, 1]\n",
      "  Token count: 5\n",
      "\n",
      "byT5 Tokenization:\n",
      "  Tokens: ['ä', '½', '\\xa0', 'å', '¥', '½', ',', ' ', 'ä', '¸', '\\x96', 'ç', '\\x95', '\\x8c']\n",
      "  IDs: [231, 192, 163, 232, 168, 192, 47, 35, 231, 187, 153, 234, 152, 143, 1]\n",
      "  Token count: 14\n",
      "\n",
      "字节映射:\n",
      "  '你' -> byte 20320 -> token_id 20323\n",
      "  '好' -> byte 22909 -> token_id 22912\n",
      "  ',' -> byte 44 -> token_id 47\n",
      "  ' ' -> byte 32 -> token_id 35\n",
      "  '世' -> byte 19990 -> token_id 19993\n",
      "  '界' -> byte 30028 -> token_id 30031\n"
     ]
    }
   ],
   "source": [
    "# Check the test cases\n",
    "for text in test_cases:\n",
    "    compare_tokenisation(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb0365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "tweets = twitter_samples.strings()\n",
    "# Show a few tweets to check \n",
    "print(tweets[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae5614f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
